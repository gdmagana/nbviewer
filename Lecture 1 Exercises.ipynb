{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture 1 Exercises.ipynb","provenance":[{"file_id":"1yfstzIo9gTK7DXPrO56V6xKW2WHc4kSC","timestamp":1617315267106}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HROj-TQX07vI"},"source":["# AI for Social Good: Lecture 1 Exercises\n","Today we will explore gender bias in natural language processing. We will learn about our first models to probe gender bias in word vectors. As a reminder, word vectors are a machine's representation of a word, learned from reading a large corpus of text to understand the context that words are used in. For example, since the words \"good\" and \"great\" are used in similar contexts, they have similar word vectors!\n","\n","These kinds of word vectors are used in everything from Google Search to Spotify recommendations, so if they are biased, this is a major problem.\n","\n","Today we will be using GloVe vectors, which are a standard type of word vector used in a variety of real-world applications. These word vectors were trained on 6 billion word tokens, sourced from Wikipedia 2014 + Gigaword5. If you're interested you can find more information [here](https://nlp.stanford.edu/projects/glove/).\n","\n","Run the below cell by highlighting it and typing Shift+Enter. This will import the required packages and download the GloVe vectors, which will take a few minutes."]},{"cell_type":"code","metadata":{"id":"n3lTwkQURp61","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648004800904,"user_tz":360,"elapsed":244518,"user":{"displayName":"Matthew Tran Radovan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10932985819394719004"}},"outputId":"aa811c0b-cc6b-4b59-c957-fcb0d5b9dc36"},"source":["import torchtext.vocab as vocab\n","import numpy as np\n","import requests\n","import zipfile\n","import io\n","\n","np.random.seed(42)\n","# Download class resources...\n","r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit1_resources.zip\")\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall()\n","\n","VEC_SIZE = 300\n","glove = vocab.GloVe(name='6B', dim=VEC_SIZE)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n","100%|█████████▉| 399999/400000 [00:46<00:00, 8537.43it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"nzLXPR8m2M3l"},"source":["## Part 1: Word Vector Exploration\n","\n","Below, we use the word vectors for 'good' and 'great' to determine the cosine similarity between them. We do the same for 'good' and 'human' (two words that are less similar). Feel free to play around and compute more similarities! Note: we have included a short helper function that retrieves the word vector for a given word."]},{"cell_type":"code","metadata":{"id":"WQfnFNjP1O0P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648005333311,"user_tz":360,"elapsed":146,"user":{"displayName":"Matthew Tran Radovan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10932985819394719004"}},"outputId":"d7794b14-b194-4f6f-bde0-21b39dc6e982"},"source":["def get_word_vector(word):\n","    return glove.vectors[glove.stoi[word]].numpy()\n","\n","def compute_cosine_similarity(word_a, word_b):\n","    a, b = get_word_vector(word_a), get_word_vector(word_b)\n","    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n","\n","print(\"good-great similarity %f\" % compute_cosine_similarity('good', 'great'))\n","print(\"good-human similarity %f\" % compute_cosine_similarity('good', 'human'))\n","print(\"good-bad similarity %f\" % compute_cosine_similarity('good', 'bad'))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["good-great similarity 0.641005\n","good-human similarity 0.313640\n","good-bad similarity 0.644522\n"]}]},{"cell_type":"markdown","metadata":{"id":"Cuq2zEP63M53"},"source":["## Part 2: Computing Logistic Regression\n","\n","Recall that the equation for linear regression prediction is computed as:\n","\n","$$\\hat{y} = wx + b$$\n","\n","With that in mind, we will write the code for the linear regression prediction computation together."]},{"cell_type":"code","metadata":{"id":"yp3xPiORSvrr","executionInfo":{"status":"ok","timestamp":1648005335157,"user_tz":360,"elapsed":4,"user":{"displayName":"Matthew Tran Radovan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10932985819394719004"}}},"source":["def compute_linear_regression(word, weights, bias):\n","    # YOUR CODE HERE (~2 lines)\n","    x = get_word_vector(word)\n","    return np.dot(weights, x) + bias\n","    # END CODE"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2deiVL0jVZP"},"source":["Let's test our code by running the following hard-coded tests:"]},{"cell_type":"code","metadata":{"id":"7tr12qD7jbSz","executionInfo":{"status":"ok","timestamp":1648005528134,"user_tz":360,"elapsed":117,"user":{"displayName":"Matthew Tran Radovan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10932985819394719004"}}},"source":["weights, bias = np.arange(VEC_SIZE) / 100., 0.\n","threshold = 1e-10\n","tests = [('good', -9.327852969836677), ('great', -2.7949289857037374), ('bad', -10.208886105293644), ('human', -1.0023533709946784)]\n","for word, pred in tests:\n","  diff = abs(pred - compute_linear_regression(word, weights, bias))\n","  assert diff <= threshold, 'Implementation incorrect for word \\'%s\\'' % word"],"execution_count":14,"outputs":[]}]}